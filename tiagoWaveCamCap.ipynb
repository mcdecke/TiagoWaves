{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tiagoWaveCamCap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR-CBgFVm8Tt"
      },
      "source": [
        "'''\n",
        "Enable Colab GPU - \n",
        "In Runtime dropdown open Change Runtime Type,\n",
        "Change Hardware accelerator dropdown to GPU\n",
        "'''\n",
        "\n",
        "!pip install pyyaml==5.1\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojiUDictnBQ0"
      },
      "source": [
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.8)\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TKIi-IinDik"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "import cv2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from math import atan2, degrees\n",
        "\n",
        "\n",
        "def angle_between(p1, p2, p3):\n",
        "    x1, y1 = p1\n",
        "    x2, y2 = p2\n",
        "    x3, y3 = p3\n",
        "    deg1 = (360 + degrees(atan2(x1 - x2, y1 - y2))) % 360\n",
        "    deg2 = (360 + degrees(atan2(x3 - x2, y3 - y2))) % 360\n",
        "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bANPS5Xmthwz"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import csv\n",
        "\n",
        "def record_video(filename='video.mp4'):\n",
        "  js = Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      // mashes together the advanced_outputs.ipynb function provided by Colab, \n",
        "      // a bunch of stuff from Stack overflow, and some sample code from:\n",
        "      // https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"green\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      // create a media recorder instance, which is an object\n",
        "      // that will let you record what you stream.\n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      // Video is a media element.  This line here sets the object which serves\n",
        "      // as the source of the media associated with the HTMLMediaElement\n",
        "      // Here, we'll set it equal to the stream.\n",
        "      video.srcObject = stream;\n",
        "      // We're inside an async function, so this await will fire off the playing\n",
        "      // of a video. It returns a Promise which is resolved when playback has \n",
        "      // been successfully started. Since this is async, the function will be \n",
        "      // paused until this has started playing. \n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      // and now, just wait for the capture button to get clicked in order to\n",
        "      // start recording\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "      // use a promise to tell it to stop recording\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "      return btoa(binaryString);\n",
        "    }\n",
        "    \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data = eval_js('recordVideo({})')\n",
        "    binary = b64decode(data)\n",
        "    with open(filename, \"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(\n",
        "        f\"Finished recording video. Saved binary under filename in current working directory: {filename}\"\n",
        "    )\n",
        "  except Exception as err:\n",
        "      # In case any exceptions arise\n",
        "      print(str(err))\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7a4E4X0IPx_"
      },
      "source": [
        "# Run the function, get the video path as saved in your notebook, and play it back here.\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_width = 300\n",
        "\n",
        "video_path = record_video()\n",
        "video_file = open(video_path, \"r+b\").read()\n",
        "\n",
        "video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vidcap = cv2.VideoCapture('video.mp4')\n",
        "success, image = vidcap.read()\n",
        "count = 1\n",
        "\n",
        "if not os.path.exists('videos'):\n",
        "    os.makedirs('videos')\n",
        "\n",
        "# Get each individual frame from the video file.\n",
        "while success:\n",
        "  cv2.imwrite(\"./videos/image_%d.jpg\" % count, image)    \n",
        "  success, image = vidcap.read()\n",
        "  print('Saved image ', count)\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "pxhbS49HvRgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-SjFS9EnR8S"
      },
      "source": [
        "joint_list = []\n",
        "\n",
        "# Detectron works better the more of you that is in frame,\n",
        "# so we can trim the video by giving the specific range.\n",
        "\n",
        "# Process each \n",
        "for j in range (80, 180):\n",
        "  im = cv2.imread(\"videos/image_\"+str(j)+\".jpg\")\n",
        "\n",
        "  # Detectron setup.\n",
        "  cfg = get_cfg()   # get a fresh new config\n",
        "  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
        "  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "  \n",
        "  if predictor(im):\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    # Show image with skeleton\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "    if(outputs[\"instances\"]):\n",
        "      rv = outputs[\"instances\"].pred_keypoints[0]\n",
        "\n",
        "      xList = []\n",
        "      yList = []\n",
        "\n",
        "      for x in rv:\n",
        "        newX = re.findall(r'\\b\\d+\\b', str(x[0]))\n",
        "        newY = re.findall(r'\\b\\d+\\b', str(x[1]))\n",
        "        xList.append(int(newX[0]))\n",
        "        yList.append(int(newY[0]))\n",
        "\n",
        "      '''\n",
        "      X,Y List order => \n",
        "      [nose, right eye, left eye, r-ear, l-ear,\n",
        "      r-shoulder, l-shoulder, r-elbow, l-elbow, r-wrist, l-wrist, \n",
        "      r-hip, l-hip, r-knee, l-knee, r-ankle, l-ankle]\n",
        "      '''\n",
        "\n",
        "      nose = [xList[0],yList[0]]\n",
        "      r_eye = [xList[1],yList[1]]\n",
        "      l_eye = [xList[2],yList[2]]\n",
        "      r_ear = [xList[3],yList[3]]\n",
        "      l_ear = [xList[4],yList[4]]\n",
        "\n",
        "      # Limb motion.\n",
        "      left_elbow = math.radians(angle_between((xList[6], yList[6]),(xList[8], yList[8]),(xList[10], yList[10])))\n",
        "      left_shoulder = math.radians(angle_between((xList[5], yList[5]),(xList[6], yList[6]),(xList[8], yList[8])))\n",
        "      right_elbow = math.pi - math.radians(angle_between((xList[5], yList[5]),(xList[7], yList[7]),(xList[9], yList[9])))\n",
        "      right_shoulder = math.pi- math.radians(angle_between((xList[6], yList[6]),(xList[5], yList[5]),(xList[7], yList[7])))\n",
        "      \n",
        "      # Head up and down, .5 is scaling.\n",
        "      head_elevation = -.5*(math.pi - math.radians(angle_between(l_ear, nose, r_ear)))\n",
        "      \n",
        "      # Head left and right.\n",
        "      r_ear_eye_len = np.linalg.norm(np.asarray(r_ear) - np.asarray(r_eye))\n",
        "      l_ear_eye_len = np.linalg.norm(np.asarray(l_ear) - np.asarray(l_eye))\n",
        "      r_eye_nose = np.linalg.norm(np.asarray(nose) - np.asarray(r_eye))\n",
        "      l_eye_nose = np.linalg.norm(np.asarray(nose) - np.asarray(l_eye))\n",
        "      head_azimuth = 1.24*(r_ear_eye_len - l_ear_eye_len )/r_ear_eye_len\n",
        "      if(abs(head_azimuth) > 1):\n",
        "        head_azimuth = -1.24*np.sign(head_azimuth)\n",
        "\n",
        "      joint_list.append([right_shoulder, right_elbow, left_shoulder, left_elbow, head_elevation, head_azimuth])\n",
        "\n",
        "print(joint_list)\n",
        "\n",
        "# order = [\"right_shoulder\", \"right_elbow\", \"left_shoulder\", \"left_elbow\", \"head_elevation\", \"head_azimuth\"]\n",
        "\n",
        "with open('movements.csv', 'w') as f: \n",
        "    write = csv.writer(f) \n",
        "    # write.writerow(order) \n",
        "    write.writerows(joint_list)\n",
        "\n",
        "print('Written to movements.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}